{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alphabet Soup's Charity Risk Assessment\n",
    "\n",
    "Alphabet Soup is a fictional nonprofit foundation that makes donations to various organizations that aim to make the world a better place by protecting the environment, improving people's well-being, etc. Unfortunately, not every investment by the foundation is impactful. The goal of this project is to come up with a data-driven solution to help ensure that the foundation's money is going to organizations that are worth donating to. The solution is to build a model to assess an organization's risk before donating to them.\n",
    "\n",
    "### Data\n",
    "\n",
    "The data contains more than 34,000 organizations that have received funding from Alphabet Soup over the years. Within this dataset are a number of columns that capture metadata about each organization:\n",
    "- `EIN` and `NAME` — Identification columns\n",
    "- `APPLICATION_TYPE` — Alphabet Soup application type\n",
    "- `AFFILIATION` — Affiliated sector of industry\n",
    "- `CLASSIFICATION` — Government organization classification\n",
    "- `USE_CASE` — Use case for funding\n",
    "- `ORGANIZATION` — Organization type\n",
    "- `STATUS` — Active status\n",
    "- `INCOME_AMT` — Income classification\n",
    "- `SPECIAL_CONSIDERATIONS` — Special consideration for application\n",
    "- `ASK_AMT` — Funding amount requested\n",
    "- `IS_SUCCESSFUL` — Was the money used effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df = pd.read_csv('data/charity_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df.columns = ['ein', 'name', 'application_type', 'affiliation', 'classification', 'use', \n",
    "              'organization', 'active', 'income', 'special_considerations', 'amount', 'successful']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique value counts\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect `amount`\n",
    "ask = df['amount']\n",
    "ask.plot(kind='hist', bins=10**np.arange(3, 10, 0.5), grid=True, figsize=(12, 4), title='Ask Amount Distribution')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Ask amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert `ein` to a categorical variable\n",
    "df['ein'] = df['ein'].astype(str)\n",
    "\n",
    "# Unique value counts for categorical variables\n",
    "cat_feats = df.dtypes[df.dtypes == object].index.tolist() # list of categorical variables\n",
    "df[cat_feats].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the top 10 labels of `classification` and `application_type`\n",
    "print(df['classification'].value_counts()[:10])\n",
    "df['application_type'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect labels of all other categorical variables\n",
    "for feat in [cat_feats[3]] + cat_feats[5:]:\n",
    "    print(df[feat].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce `classification` to the top 3 and an other label\n",
    "df['classification'] = df['classification'].apply(lambda c: c if c in ['C1000', 'C2000', 'C1200'] else '0ther')\n",
    "\n",
    "# Create a dichotomous variable indicating whether the `application_type` is T3\n",
    "df['application_T3'] = (df['application_type'] == 'T3').astype(int)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable reducing `affiliation` to the top 2 and an other label\n",
    "df['affiliation'] = df['affiliation'].apply(lambda a: a if a in ['Independent', 'CompanySponsored'] else '0ther')\n",
    "\n",
    "# Create a variable reducing `use` to the top 2 and an other label\n",
    "df['use'] = df['use'].apply(lambda a: a if a in ['Preservation', 'ProductDev'] else '0ther')\n",
    "\n",
    "# Create a variable reducing `organization` to the top 2 and an other label\n",
    "df['organization'] = df['organization'].apply(lambda a: a if a in ['Trust', 'Association'] else '0ther')\n",
    "\n",
    "# Create a dichotomous variable indicating whether there were `special_considerations`\n",
    "df['special'] = (df['special_considerations'] == 'Y').astype(int)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine `income` labels into 3 buckets: 0, <100k, 100k+\n",
    "labels_lt100k = ['1-9999', '10000-24999', '25000-99999'] # labels for <100k bucket\n",
    "df['income'] = df['income'].apply(lambda i: '<100k' if i in labels_lt100k else i) # <100k bucket\n",
    "df['income'] = df['income'].apply(lambda i: i if i in ['0', '<100k'] else '100k+') # 100k+ bucket\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique value counts for categorical variables\n",
    "cols_to_encode = cat_feats[3:-2]\n",
    "df[cols_to_encode].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of categorical features\n",
    "for feat in cols_to_encode:\n",
    "    print(df[feat].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "df_ohe = pd.DataFrame(ohe.fit_transform(df[cols_to_encode]), \n",
    "                      columns=ohe.get_feature_names(cols_to_encode)).astype(int)\n",
    "df_ohe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for `income`\n",
    "df = pd.get_dummies(df, columns=['income'])\n",
    "\n",
    "# Merge data\n",
    "df_merged = df.merge(df_ohe, left_index=True, right_index=True)\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical columns\n",
    "cat_feats.remove('income') # `income` already dropped\n",
    "df_merged.drop(columns=cat_feats, inplace=True)\n",
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform `amount`\n",
    "df_merged['transformed_amount'] = np.log10(df_merged['amount']) / 10 # log-transform and divide by 10\n",
    "df_merged.drop(columns='amount', inplace=True) # drop `amount`\n",
    "\n",
    "# Plot transformed `amount`\n",
    "df_merged['transformed_amount'].hist()\n",
    "plt.xlabel('Log10(amount) / 10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect values\n",
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature/target split\n",
    "X = df_merged.drop(columns='successful')\n",
    "y = df_merged['successful']\n",
    "\n",
    "# Inspect target\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=24)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "lr = LogisticRegression(max_iter=1e4, random_state=1)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "print('Training set accuracy:', lr.score(X_train, y_train))\n",
    "print('Test set accuracy:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "sorted(list(zip(lr.coef_[0], X_train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "print('Training set accuracy:', rf.score(X_train, y_train))\n",
    "print('Test set accuracy:', rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to tune\n",
    "params = {'max_depth': [5, 10, None],\n",
    "          'min_samples_split': [2, 8, 32],\n",
    "          'min_samples_leaf': [1, 4, 16],\n",
    "          'max_features': ['sqrt', 0.5, None],\n",
    "          'max_samples': [0.5, 0.75, None]}\n",
    "\n",
    "# Grid search\n",
    "rf_search = GridSearchCV(RandomForestClassifier(random_state=100), params, cv=3, verbose=2, n_jobs=-1)\n",
    "rf_search.fit(X_train, y_train)\n",
    "print('Best score:', rf_search.best_score_)\n",
    "rf_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model\n",
    "rf_best = rf_search.best_estimator_\n",
    "print('Training set accuracy:', rf_best.score(X_train, y_train))\n",
    "print('Test set accuracy:', rf_best.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "sorted(list(zip(rf_best.feature_importances_, X_train.columns)), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic NN\n",
    "bnn = keras.models.Sequential(name='risk_basic')\n",
    "\n",
    "# Parameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units = input_dim * 2\n",
    "output_units = 1\n",
    "\n",
    "# Layers\n",
    "bnn.add(keras.layers.Dense(units=hidden_units, activation='relu', input_dim=input_dim)) # input & hidden layer\n",
    "bnn.add(keras.layers.Dense(units=output_units, activation='sigmoid')) # output layer\n",
    "\n",
    "bnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint path\n",
    "bnn_cdir = 'checkpoints/bnn/'\n",
    "os.makedirs(bnn_cdir, exist_ok=True) # create directory for checkpoints\n",
    "bnn_cpath = bnn_cdir + 'bnn_cp{epoch}.hdf5'\n",
    "\n",
    "# Checkpoint callback\n",
    "cfreq = X_train.shape[0] * 10 # save cp every 10 epochs\n",
    "bnn_ccallback = keras.callbacks.ModelCheckpoint(bnn_cpath, save_weights_only=True, save_freq=cfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "bnn.compile('adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "bnn.fit(X_train, y_train, epochs=200, callbacks=[bnn_ccallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "bnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential NN\n",
    "nn1 = keras.models.Sequential(name='risk1')\n",
    "\n",
    "# Parameters\n",
    "h1_units = 12\n",
    "h2_units = 6\n",
    "\n",
    "# Layers\n",
    "nn1.add(keras.layers.Dense(units=h1_units, activation='relu', input_dim=input_dim)) # input & 1st hidden layer\n",
    "nn1.add(keras.layers.Dense(units=h2_units, activation='relu')) # 2nd hidden layer\n",
    "nn1.add(keras.layers.Dense(units=output_units, activation='sigmoid')) # output layer\n",
    "        \n",
    "nn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint path\n",
    "nn1_cdir = 'checkpoints/nn1/'\n",
    "os.makedirs(nn1_cdir, exist_ok=True) # create directory for checkpoints\n",
    "nn1_cpath = nn1_cdir + 'nn1_cp{epoch}.hdf5'\n",
    "\n",
    "# Checkpoint callback\n",
    "nn1_ccallback = keras.callbacks.ModelCheckpoint(nn1_cpath, save_weights_only=True, save_freq=cfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "nn1.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
    "\n",
    "# Train model\n",
    "nn1.fit(X_train, y_train, epochs=200, callbacks=[nn1_ccallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "nn1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential NN\n",
    "nn2 = keras.models.Sequential(name='risk2')\n",
    "\n",
    "# Parameters\n",
    "h1_units = 12\n",
    "h2_units = 6\n",
    "h3_units = 6\n",
    "\n",
    "# Layers\n",
    "nn2.add(keras.layers.Dense(units=h1_units, activation='relu', input_dim=input_dim)) # input & 1st hidden layer\n",
    "nn2.add(keras.layers.Dense(units=h2_units, activation='relu')) # 2nd hidden layer\n",
    "nn2.add(keras.layers.Dense(units=h3_units, activation='relu')) # 3rd hidden layer\n",
    "nn2.add(keras.layers.Dense(units=output_units, activation='sigmoid')) # output layer\n",
    "        \n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint path\n",
    "nn2_cdir = 'checkpoints/nn1/'\n",
    "os.makedirs(nn2_cdir, exist_ok=True) # create directory for checkpoints\n",
    "nn2_cpath = nn2_cdir + 'nn2_cp{epoch}.hdf5'\n",
    "\n",
    "# Checkpoint callback\n",
    "nn2_ccallback = keras.callbacks.ModelCheckpoint(nn2_cpath, save_weights_only=True, save_freq=cfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "nn2.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
    "\n",
    "# Train model\n",
    "nn2.fit(X_train, y_train, epochs=100, callbacks=[nn2_ccallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "nn2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 4 columns\n",
    "cols_to_drop = ['active', 'special', 'classification_C1200', 'classification_C2000']\n",
    "X12_train = X_train.drop(columns=cols_to_drop)\n",
    "X12_test = X_test.drop(columns=cols_to_drop)\n",
    "X12_train.shape, X12_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential NN\n",
    "nn3 = keras.models.Sequential(name='risk3')\n",
    "\n",
    "# Parameters\n",
    "input_dim = X12_train.shape[1]\n",
    "h1_units = 8\n",
    "h2_units = 4\n",
    "h3_units = 4\n",
    "\n",
    "# Layers\n",
    "nn3.add(keras.layers.Dense(units=h1_units, activation='relu', input_dim=input_dim)) # input & 1st hidden layer\n",
    "nn3.add(keras.layers.Dense(units=h2_units, activation='relu')) # 2nd hidden layer\n",
    "nn3.add(keras.layers.Dense(units=h3_units, activation='relu')) # 3rd hidden layer\n",
    "nn3.add(keras.layers.Dense(units=output_units, activation='sigmoid')) # output layer\n",
    "        \n",
    "nn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint path\n",
    "nn3_cdir = 'checkpoints/nn3/'\n",
    "os.makedirs(nn3_cdir, exist_ok=True) # create directory for checkpoints\n",
    "nn3_cpath = nn3_cdir + 'nn3_cp{epoch}.hdf5'\n",
    "\n",
    "# Checkpoint callback\n",
    "nn3_ccallback = keras.callbacks.ModelCheckpoint(nn3_cpath, save_weights_only=True, save_freq=cfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "nn3.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
    "\n",
    "# Train model\n",
    "nn3.fit(X12_train, y_train, epochs=100, callbacks=[nn2_ccallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "nn3.evaluate(X12_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minds] *",
   "language": "python",
   "name": "conda-env-minds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
